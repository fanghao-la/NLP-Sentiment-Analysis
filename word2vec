__author__ = 'fang'
#import sys
#reload(sys)
#sys.setdefaultencoding('utf8')

import pandas as pd
train = pd.read_csv("C:/Users/pp/PycharmProjects/bag_of_words/labeledTrainData.tsv", delimiter="\t",
                    quoting=3, header=0)
test = pd.read_csv("C:/Users/pp/PycharmProjects/bag_of_words/testData.tsv", delimiter="\t",
                   quoting=3, header=0)
unlabeled_train = pd.read_csv("C:/Users/pp/PycharmProjects/bag_of_words/unlabeledTrainData.tsv", delimiter="\t",
                              quoting=3, header=0)
print "read %d labeled train reviews, %d unlabeled train reviews and %d test reviews \n" \
      % (train["review"].size, unlabeled_train["review"].size, test["review"].size)

from bs4 import BeautifulSoup as BS
import re
from nltk.corpus import stopwords
def review_to_wordlist(raw_review, remove_stopwords=False):
    review_text = BS(raw_review).get_text()
    review = re.sub("[^a-zA-Z0-9]", " ", review_text)
    review = review.lower().split()
    if remove_stopwords:
        stop_words = set(stopwords.words("english"))
        review = [w for w in review if not w in stop_words]
    return review

import nltk.data
#nltk.download()
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

def review_to_sentences(review, tokenizer, remove_stopwords=False):
    raw_sentences = tokenizer.tokenize(review.strip().decode("utf8"))
    sentences = []
    for raw_sentence in raw_sentences:
        if len(raw_sentence)>0:
            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))

    return sentences

sentences = []
print "Parsing sentences from training set"
for review in train["review"]:
    sentences += review_to_sentences(review, tokenizer)

print "Parsing sentences from unlabeled training set"
for review in unlabeled_train["review"][0]:
    sentences += review_to_sentences(review, tokenizer)

print len(sentences)
type(sentences)
type(sentences[0])

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
num_features = 300
min_word_count = 40
num_workers = 4
context = 10
downsampling = 1e-3

import cython
from gensim.models import word2vec
print "Training word2vec model..."
model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count,
                          window=context, sample=downsampling)
model.init_sims(replace=True)
model_name = "300features_40minwords_10context"
model.save(model_name)

model.doesnt_match("man woman child kitchen".split())
model.doesnt_match("france england germany berlin".split())
model.doesnt_match("paris berlin london austria".split())

model.most_similar("man")
#print "Parsing sentences from test set"
#for review in test["review"][0]:
#    sentences += review_to_sentences(review, tokenizer)

from gensim.models import Word2Vec
model = Word2Vec.load("300features_40minwords_10context")
type(model.syn0)
model.syn0.shape
model["flower"]

import numpy as np
from sklearn.ensemble import RandomForestClassifier

###########################################################
#def makeFeatureVec(words, model, num_features):
#    featureVec = np.zeros((num_features,), dtype="float32")
#    nwords = 0
#    index2word_set = set(model.index2word)
#    for word in words:
#        if word in index2word_set:
#            nwords = nwords + 1
#            featureVec = np.add(featureVec, model[word])
#    featureVec = np.divide(featureVec, nwords)
#    return featureVec

#def getAvgFeatureVecs(reviews, model, num_feature):
#    counter = 0
#    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype="float32")
#    for review in reviews:
#        if counter%5000 == 0.:
#            print "Review %d of %d" % (counter, len(reviews))
#        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)
#        counter = counter + 1
#    return reviewFeatureVecs

clean_train_reviews = []
for review in train["review"]:
    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))

#trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)

#print "Creating average feature vecs for test reviews"
clean_test_reviews = []
for review in test["review"]:
    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords = True))

#testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)

#forest = RandomForestClassifier(n_estimators = 100)
#forest = forest.fit(trainDataVecs, train["sentiment"])
#result = forest.predict(testDataVecs)
#output = pd.DataFrame(data={"id": test["id"], "sentiment": result})
#output.to_csv("Word2Vec_AverageVectors.csv", index=False, quoting=3)

###############################################################################
from sklearn.cluster import KMeans
import time

start = time.time()

word_vectors = model.syn0
num_clusters = word_vectors.shape[0]/5
kmeans_clustering = KMeans(n_clusters = num_clusters)
idx = kmeans_clustering.fit_predict(word_vectors)

end = time.time()
elapsed = end - start
print "Time taken for K Means clustering:", elapsed, "seconds."

word_centroid_map = dict(zip(model.index2word, idx))

def create_bag_of_centroids(wordlist, word_centroid_map):
    num_centroids = max(word_centroid_map.values()) + 1
    bag_of_centroids = np.zeros(num_centroids, dtype="float32")
    for word in wordlist:
        if word in word_centroid_map:
            index = word_centroid_map[word]
            bag_of_centroids[index] += 1
    return bag_of_centroids

train_centroids = np.zeros((train["review"].size, num_clusters), dtype="float32")
counter = 0
for review in clean_train_reviews:
    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)
    counter += 1

test_centroids = np.zeros((test["review"].size, num_clusters), dtype="float32")
counter = 0
for review in clean_test_reviews:
    test_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)
    counter += 1

forest = RandomForestClassifier(n_estimators=100)
print "Fitting a random forest to labeled training data..."
forest = forest.fit(train_centroids, train["sentiment"])
result = forest.predict(test_centroids)

output = pd.DataFrame(data={"id":test["id"], "sentiment":result})
output.to_csv("BagOfCentroids.csv", index=False, quoting=3)
